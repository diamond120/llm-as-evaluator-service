
from enum import Enum
import os
from rich.markdown import Markdown
from rich import print as md
from typing import Annotated, List, Sequence, TypedDict, Union
from langchain.output_parsers import PydanticOutputParser
import contextlib
import io

from llm_failover import ChatFailoverLLM
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import BaseMessage
import operator
from langgraph.prebuilt.tool_executor import ToolExecutor
from langgraph.graph import END, StateGraph
import json

from langchain_core.messages import (
    AIMessage,
    BaseMessage,
    ChatMessage,
    FunctionMessage,
    HumanMessage,
)
from langchain.tools.render import format_tool_to_openai_function
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import END, StateGraph
from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.tools import tool
from pydantic import BaseModel, Field

class Severity(Enum):
    CRITICAL = "Critical"
    MEDIUM = "Medium"
    LOW = "Low"

class Issue(BaseModel):
    """Represents a specific issue found during code review."""

    cell_position: int = Field(
        ..., description="The position of the cell where the issue was found."
    )
    what: str = Field(..., description="A brief description of the issue.")
    why: str = Field(..., description="Explanation of why this is an issue.")
    where: str = Field(
        ...,
        description="Specific location within the cell where the issue can be found.",
    )
    severity: Severity = Field(
        ...,
        description="The severity level of the issue, categorized as Critical, Medium, or Low. Critical issues majorly decrease the usefulness of the Assistant code replies for the human user. Medium severity issues have a strong influence on the conversation flow and usefulness. Low severity issues have almost no influence on the overall score but could improve the quality if addressed.",
    )
    fix: str = Field(
        ..., description="Suggested fix for the issue in an executive summary fashion."
    )


class NotebookWiseFeedback(BaseModel):
    """Represents the outcome of a code review task."""

    scratchpad: str = Field(
        ...,
        description="Place for you to think. Think before issues and score creation. Be concise. Analyze the text to achieve your goal. Always think before looking for issues!",
    )
    issues: list[Issue] = Field(
        ...,
        description="List of issues identified in the code review, categorized by severity.",
    )
    scoring_explanation: str = Field(
        ...,
        description="Explanation of the logic behind scoring this conversation, using the grading rules provided.",
    )
    score: int | None = Field(
        ...,
        description="A score between 1 and 5 that reflects the quality of the code, where 1 is the worst and 5 is the best, based on the criteria outlined in the grading rules.",
    )
    

#define graph state
class AgentState(TypedDict):
    chat_history: list[BaseMessage]
    messages: Annotated[Sequence[BaseMessage], operator.add]
    sender: str
    user_config:dict


main_prompt1 = """Here's a shortened prompt that includes all the additional steps while maintaining clarity:

---

# IDENTITY

You are Codia, an AI expert in Python programming. You excel in evaluating and refining code generated by AI assistants.

# GOALS

Evaluate and provide feedback on code in a Jupyter notebook conversation between a human user and an AI Assistant.

# RULES

Focus on:
- Code Correctness
- Code Efficiency
- Best Practices
- Code Readability
- Code Style Consistency
- Code Purpose and Usefulness for User Request Satisfaction

**1. Code for Review**
- Analyze code generated by the LLM Assistant.
- Exclude human user input and non-code text by the AI Assistant.
- Consider code comments as part of the code.

**2. Evaluation Criteria**
- Correctness: No bugs or errors.
- Efficiency: Optimized for performance.
- Best Practices: Adheres to programming conventions and guidelines.
- Readability: Clear and understandable with appropriate naming and comments.
- Consistency: Aligns with the Assistant's programming identity and user context.
- Completeness: Fully satisfies the user request without further interaction needed.

**3. Review Guidelines**
- Provide specific, objective feedback.
- Focus on substantial issues affecting code quality.

# Grading Score:
```
### 5 - Excellent
- Well Formatted, Correct, Optimal, Highly Readable, Useful
- Fully satisfies user request

### 4 - Good
- Correct but can be slightly optimized

### 3 - Acceptable
- Correct but can be significantly improved or lacks readability

### 2 - Needs Improvement
- Incorrect, out of scope, or has syntax errors

### 1 - Poor
- Incomplete or missing code necessary for user request
```

**Steps for Evaluation:**
1. Review the entire conversation to identify testable code.
2. Write up test cases (happy path and one edge cases) to validate correctness and efficiency, feel free to write as many as possible.
3. Use the `python_repl` function to run test cases and print feedback.
4. Ensure functions, variables and other objects required for the tests to run are defined so it can be executable within `python_repl`.
5. Take note that code indented under if __name__ == "__main__": won't run.
6. If external libraries are needed, verify their existence with the `tavily_tool`.
7. Compile all observations and generate final evaluation results.

**Refocus:**
- Review code, not contextual information.
- If code is unnecessary and user request is fully satisfied without it, do not rate.
- If necessary code is missing, rate as 1.
- Only handle Python code; if the code isn't Python, evaluate and direct to save_output.

---
""" 



main_prompt = """# IDENTITY

You are an AI named Codia. You have extensive knowledge and skill in programming languages, especially Python. You are aware of the best practices used in programming, have an extensive extensive experience in algorithms, data structures and overall computer science.

You are a concise expert in evaluating and refining the code generated by an AI assistant based on a Large Language Model.

# GOALS

Your task is to evaluate and provide feedback for a conversation between a human user and an AI Assistant that is based on the latest large language model architecture.
Focus of your evaluation is code in the replies generated by the AI Assistant only. The conversation environment is a Jupyter notebook, thus things that are run in other cells, are available in the next cells.

# RULES

Attributes to consider:
- Code Correctness
- Code Efficiency
- Best Practices
- Code Readability
- Code style Consistency
- Code purpose and usefulness for user request satisfaction

**1. Identification of Code for Review**
- Target for analysis: Code generated by the LLM Assistant in a reply to the User within a Jupyter notebook exchange.
- Exclude analysis of human user input for focused improvement on LLM-generated content.
- Exclude LLM Assistant text content that is not related to the code, only review code snippets and code cells. Text is for context and reasoning/explanation only, you can assess meaning of the text in relation to the code.
- Exclude concerns about code explanation in the text parts if they are not comments inside the code, as it will be covered by other reviewers.

**2. Evaluation Criteria Definitions**
- Correctness: The code must be devoid of bugs and errors.
- Efficiency: The code must be optimized for maximum performance.
- Best Practices: The code must adhere to established programming conventions, techniques, and guidelines.
- Readability: The code must be easily comprehensible, with suitable naming conventions and comments where complexity demands.
- Consistency: The code must be consistent with the Assistant's programming identity and the context of the user interaction.
- Completeness of the conversation as a whole: was user request satisfied or does conversation still needs more interactions(very bad)?

**3. Review Guidelines**
- Avoid general praise observations: Be specific and objective in your feedback.
- Avoid nitpicky/subjective criticism: Focus on substantial issues that affect the code quality.

# Grading score rules:
```
### 5 - Excellent
- Well Formatted
- Correct
- Optimal
- Highly readable
- Useful
- conversation must be complete ending in user request full satisfaction

### 4 - Good
- Correct but can be slightly optimized in terms of approach / speed / readability

### 3 - Acceptable
- The code is correct but can be significantly improved.
- The code is not readable.

### 2 - Needs Improvement
- The code is incorrect / out of scope / has syntax errors.
- Looks like itâ€™s copied from ChatGPT - robotic, no personality, inhuman.

### 1 - Poor
- Incomplete or missing Code, but is required or implied by context of the interaction to make it useful aka did not satisfy user's request and desire
```


**Steps for Evaluation:**
1. Review the entire conversation to identify testable code.
2. Write up test cases (happy path and one edge cases) to validate correctness and efficiency, feel free to write as many as possible.
3. Use the `python_repl` function to run test cases and print feedback.
4. Ensure functions, variables and other objects required for the tests to run are defined so it can be executable within `python_repl`.
5. Take note that code indented under if __name__ == "__main__": won't run.
6. If external libraries are needed, verify their existence with the `tavily_tool`.
7. Compile all observations and generate final evaluation results.

# REFOCUS:
- You are a code reviewer, not a language and contextual information content reviewer Do not mention issues not related to your purpose.
- If the code was **unnecessary** aka user request FULLY satisfied without it, it can be absent and thus must receive null.
- If code from assistant is necessary by the conversation flow to satisfy user's request but it is not there - score it as 1, do not mark as 5.
- As you are giving a rating to a reply from a perfect AI Assistant, each issue decreases the rating/score significantly. If there is at least one of medium issue - 3 is max rating already and must go lower if more or issues are worse.

"""


#SCHEMA INSTRCTIONS
pydantic_parser = PydanticOutputParser(pydantic_object=NotebookWiseFeedback)
format_instructions = pydantic_parser.get_format_instructions()
tavily_tool = TavilySearchResults(max_results=5)






@tool
def python_repl(code: Annotated[str, "The python code to execute."]):
    """Use this to execute python code when needed. If you want to see the output of a value,
    you should print it out with `print(...)`. This is visible to the user and you.
    """
    # Create StringIO objects to capture stdout and stderr
    stdout = io.StringIO()
    stderr = io.StringIO()

    # Use context managers to redirect stdout and stderr to our StringIO objects
    with contextlib.redirect_stdout(stdout), contextlib.redirect_stderr(stderr):
        try:
            # Use exec to execute the code
            exec(code, locals())
            result = stdout.getvalue()
            error = stderr.getvalue()
        except Exception as e:
            # If an error occurs during execution, return the error message
            return f"Failed to execute. Error: {repr(e)}"

    # If no errors occurred, return the output
    if error:
        return f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}\nStderr: {error}"
    else:
        return f"Successfully executed:\n```python\n{code}\n```\nStdout: {result}"


all_tools = [
            tavily_tool,
            python_repl,
        ]


#Simple Node
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "{main_prompt}"
            "Finally when you are done and run all neccessary tests, provide your final evaluation results"
            "Your final results MOST and ALWAYS be provided as a JSON that matches with these schema as described below: \n {schema}"
        
        ),
        MessagesPlaceholder(variable_name="messages")
    ])

llm = ChatFailoverLLM(initial_provider="openai_api", initial_model="gpt-4o",  model_kwargs = {"response_format":{"type": "json_object"}}, temperature= 0.8)
functions = [format_tool_to_openai_function(t) for t in all_tools]
prompt = prompt.partial(main_prompt = main_prompt)
prompt = prompt.partial(schema = format_instructions )
prompt = prompt.partial(tool_names=", ".join([tool.name for tool in all_tools]))
model1 = prompt | llm.bind_functions(functions)


def main_node(state):
    out = model1.invoke(state)
    return {
        "messages":[out],
        "sender": "main_node",
    }

#Tool Executor
tool_executor = ToolExecutor(all_tools)


def tool_node(state):

    """This runs tools in the graph

    It takes in an agent action and calls that tool and returns the result."""
    messages = state["messages"]
  
    # Based on the continue condition
    # we know the last message involves a function call
    last_message = messages[-1]
    # We construct an ToolInvocation from the function_call
    try:
        tool_input = json.loads(
            last_message.additional_kwargs["function_call"]["arguments"]
        )
    except:
        tool_input = {"code":last_message.additional_kwargs["function_call"]["arguments"]} #sometimes the actual code is sent as a string instead of {code:"code"}
    # We can pass single-arg inputs by value
    if len(tool_input) == 1 and "__arg1" in tool_input:
        tool_input = next(iter(tool_input.values()))
    tool_name = last_message.additional_kwargs["function_call"]["name"]
    action = ToolInvocation(
        tool=tool_name,
        tool_input=tool_input,
    )
    # We call the tool_executor and get back a response
    response = tool_executor.invoke(action)
    # We use the response to create a FunctionMessage
    function_message = FunctionMessage(
        content=f"{tool_name} response: {str(response)}", name=action.tool
    )
    # We return a list, because this will get added to the existing list
    return {"messages": [function_message]}


# Either agent can decide to end
def router(state):
    # This is the router
    messages = state["messages"]
    sender = state["sender"]
    last_message = messages[-1]
    
    if "function_call" in last_message.additional_kwargs:
        return "call_tool" #irrespective of the sender
    
    return "continue"




workflow = StateGraph(AgentState)
workflow.add_node("main_node", main_node)
workflow.add_node("call_tool", tool_node)


workflow.add_conditional_edges(
    "main_node",
    router,
    {"continue": END, "call_tool": "call_tool"},
)

workflow.add_conditional_edges(
    "call_tool",
    lambda x: x["sender"],
    {
        "main_node": "main_node"
    },
)

workflow.set_entry_point("main_node")
graph = workflow.compile()


# #DRAW GRAPH
# from langchain_core.runnables.graph import CurveStyle, NodeColors, MermaidDrawMethod
# from IPython.display import display, HTML, Image

# display(
#     Image(
#         graph.get_graph().draw_mermaid_png(
#             draw_method=MermaidDrawMethod.API,
#         )
#     )
# )


    
def run(input_message):
   
    for s in graph.stream(input_message, {"recursion_limit": 20}):
        print("AGENT:", s)
        agent = list(s.keys())[0]
        content = s[agent]["messages"][-1].content
        
        if agent == "main_node":
            #check if it is trying to call a function/tool
            if "function_call" in s[agent]["messages"][-1].additional_kwargs:
                function_being_called = s[agent]["messages"][-1].additional_kwargs['function_call']['name']
                args = s[agent]["messages"][-1].additional_kwargs['function_call']['arguments']
                content = f"I am calling the function `{function_being_called}` with the following arguments: {args}"
                content = Markdown(content)
                md(content)
            else:
                try:
                    content = str(json.loads(content))
                except:
                    pass
                content = Markdown(content)
                md(content)
        else:
            content = Markdown(content)
            md(content)
            
    return   s[agent]["messages"][-1].content
    